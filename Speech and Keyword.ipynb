{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import speech_recognition as sr\n",
    "import pyaudio\n",
    "from googletrans import Translator\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.util import ngrams\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import pytextrank\n",
    "from rake_nltk import Rake\n",
    "import re, string\n",
    "import en_core_web_lg\n",
    "import time\n",
    "from math import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transcribing an audio file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "r=sr.Recognizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'alternative': [{'transcript': 'the salesman old beer drinkers it takes hi to bring out the order I call dip restore selfinvest a salt a call this find him because of my favourite is just for food is Bihar cross bun',\n",
       "   'confidence': 0.84945077},\n",
       "  {'transcript': 'the salesman old beer drinkers it takes hi to bring out the order I called research help in West a salt a call this find him because of my favourite is just for food is Bihar cross bun'},\n",
       "  {'transcript': 'the salesman old beer drinkers it takes hi to bring out the older I call dip research help in West a salt a call this find him because of my favourite is just for food is Bihar cross bun'},\n",
       "  {'transcript': 'the salesman old beer drinkers it takes hi to bring out the older I called research help in West a salt a call this find him because of my favourite is just for food is Bihar cross bun'},\n",
       "  {'transcript': 'the salesman old beer drinkers it takes hi to bring out the order I called research help invest a salt a call this find him because of my favourite is just for food is Bihar cross bun'}],\n",
       " 'final': True}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "audiofile=sr.AudioFile('harvard.wav')\n",
    "with audiofile as source:\n",
    "    audio=r.record(source)\n",
    "    #audio=r.record(source,duration=6)\n",
    "queries = r.recognize_google(audio,show_all=True)\n",
    "queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the salesman old beer drinkers it takes hi to bring out the order I call dip restore selfinvest a salt a call this find him because of my favourite is just for food is Bihar cross bun'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = queries[\"alternative\"][0][\"transcript\"]\n",
    "query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using the microphone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'r = sr.Recognizer()\\nm = sr.Microphone()\\n\\ntry:\\n    with m as source: r.adjust_for_ambient_noise(source)\\n    print(\"Set minimum energy threshold to {}\".format(r.energy_threshold))\\n    print(\"Say something!\")\\n    with m as source: audio = r.listen(source)\\n    #print(audio)\\n    print(\"Got it! Now to recognize it...\")\\n    try:\\n        value = r.recognize_google(audio,show_all=False,language = \\'en-US\\') #fr-FR, hi-IN,kn-IN,ta-IN\\n        #value = r.recognize_sphinx(audio)\\n        #print(\"You said {}\".format(value))\\n        print(value)\\n    except sr.UnknownValueError:\\n        print(\"Didn\\'t catch that\")\\nexcept KeyboardInterrupt:\\n    pass'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''r = sr.Recognizer()\n",
    "m = sr.Microphone()\n",
    "\n",
    "try:\n",
    "    with m as source: r.adjust_for_ambient_noise(source)\n",
    "    print(\"Set minimum energy threshold to {}\".format(r.energy_threshold))\n",
    "    print(\"Say something!\")\n",
    "    with m as source: audio = r.listen(source)\n",
    "    #print(audio)\n",
    "    print(\"Got it! Now to recognize it...\")\n",
    "    try:\n",
    "        value = r.recognize_google(audio,show_all=False,language = 'en-US') #fr-FR, hi-IN,kn-IN,ta-IN\n",
    "        #value = r.recognize_sphinx(audio)\n",
    "        #print(\"You said {}\".format(value))\n",
    "        print(value)\n",
    "    except sr.UnknownValueError:\n",
    "        print(\"Didn't catch that\")\n",
    "except KeyboardInterrupt:\n",
    "    pass'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Translating the text to English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected(lang=en, confidence=1.0)\n",
      "the salesman old beer drinkers it takes hi to bring out the order I call dip restore selfinvest a salt a call this find him because of my favourite is just for food is Bihar cross bun\n"
     ]
    }
   ],
   "source": [
    "translator = Translator()\n",
    "source_details = translator.detect(query)\n",
    "print(source_details)\n",
    "if source_details.lang!='en':\n",
    "    query = translator.translate(query, src = source_details.lang, dest = 'en')\n",
    "    print(query)\n",
    "    print(query.text)\n",
    "else:\n",
    "    print(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keyword Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "stopw = set(stopwords.words('english'))\n",
    "remove_punctuation_map = dict((ord(char), None) for char in string.punctuation)\n",
    "sp = en_core_web_lg.load()\n",
    "r = Rake()\n",
    "textrank = pytextrank.TextRank()\n",
    "sp.add_pipe(textrank.PipelineComponent, name=\"textrank\", last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "contraction = {\n",
    "    \"ain't\": \"is not\",\n",
    "    \"aren't\": \"are not\",\n",
    "    \"can't\": \"cannot\", \n",
    "    \"can't've\": \"cannot have\", \n",
    "    \"'cause\": \"because\",\n",
    "    \"could've\": \"could have\",\n",
    "    \"couldn't\": \"could not\",\n",
    "    \"couldn't've\": \"could not have\",\n",
    "    \"didn't\": \"did not\",\n",
    "    \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \n",
    "    \"hadn't've\": \"had not have\", \n",
    "    \"hasn't\": \"has not\",\n",
    "    \"haven't\": \"have not\", \n",
    "    \"he'd\": \"he would\", \"he'd've\": \"he would have\", \"he'll\": \"he will\", \n",
    "    \"he'll've\": \"he he will have\", \"he's\": \"he is\", \"how'd\": \"how did\", \n",
    "    \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\", \n",
    "    \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \n",
    "    \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \n",
    "    \"i'd\": \"i would\", \"i'd've\": \"i would have\", \"i'll\": \"i will\", \n",
    "    \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \n",
    "    \"isn't\": \"is not\", \"it'd\": \"it would\", \"it'd've\": \"it would have\", \n",
    "    \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \n",
    "    \"let's\": \"let us\", \"ma'am\": \"madam\", \"mayn't\": \"may not\", \n",
    "    \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \n",
    "    \"must've\": \"must have\", \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \n",
    "    \"needn't\": \"need not\", \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\", \n",
    "    \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\",\n",
    "    \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\", \"she'd\": \"she would\", \n",
    "    \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \n",
    "    \"she's\": \"she is\", \"should've\": \"should have\", \"shouldn't\": \"should not\", \n",
    "    \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\", \n",
    "    \"this's\": \"this is\",\n",
    "    \"that'd\": \"that would\", \"that'd've\": \"that would have\",\"that's\": \"that is\", \n",
    "       \"there'd\": \"there would\", \"there'd've\": \"there would have\",\"there's\": \"there is\", \n",
    "       \"they'd\": \"they would\", \"they'd've\": \"they would have\", \"they'll\": \"they will\", \n",
    "       \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \n",
    "       \"to've\": \"to have\", \"wasn't\": \"was not\", \"we'd\": \"we would\", \n",
    "       \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \n",
    "       \"we're\": \"we are\", \"we've\": \"we have\", \"weren't\": \"were not\", \n",
    "       \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\", \n",
    "       \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \n",
    "       \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\", \n",
    "       \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \n",
    "       \"who's\": \"who is\", \"who've\": \"who have\", \"why's\": \"why is\", \n",
    "       \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \n",
    "       \"won't've\": \"will not have\", \"would've\": \"would have\", \"wouldn't\": \"would not\", \n",
    "       \"wouldn't've\": \"would not have\", \"y'all\": \"you all\", \"y'all'd\": \"you all would\",\n",
    "       \"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\n",
    "       \"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \n",
    "       \"you'll've\": \"you will have\", \"you're\": \"you are\", \"you've\": \"you have\"}\n",
    "\n",
    "def clean(text):\n",
    "    text = text.lower()\n",
    "    temp = \"\"\n",
    "    for i in text.split():\n",
    "        try:\n",
    "            temp+=contraction[i]+' '\n",
    "        except:\n",
    "            temp+= i+' '\n",
    "    text = temp.strip()\n",
    "    text = text.lower().translate(remove_punctuation_map)\n",
    "    text = re.sub(\"[^a-zA-Z#]\",\" \",text)\n",
    "    text = re.sub(r\"[^A-Za-z0-9^,!.\\/'+-=]\", \" \", text)\n",
    "    text = re.sub(r\"what's\", \"what is\", text)\n",
    "    text = re.sub(r\",\", \"\", text)\n",
    "    text = re.sub(r\"\\.\", \"\", text)\n",
    "    text = re.sub(r\"!\", \"!\", text)\n",
    "    text = re.sub(r\"\\/\", \"\", text)\n",
    "    text = re.sub(r\"'\", \"\", text)\n",
    "    text = re.sub(r\"(\\d+)(k)\", r\"\\g<1>000\", text)\n",
    "    text = re.sub(r\":\", \":\", text)\n",
    "    text = re.sub(r' +',' ',text)\n",
    "    return text.strip()\n",
    "\n",
    "def stopwordremoval(text):\n",
    "    text = word_tokenize(text)\n",
    "    text = [i for i in text if i not in stopw]\n",
    "    return \" \".join(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pp_set(text, op):\n",
    "    key_tokenized_sentences = sent_tokenize(text)\n",
    "    key_tokenized_words = word_tokenize(text)\n",
    "    if op == \"token_sent\":\n",
    "        return key_tokenized_sentences\n",
    "    elif op == \"token_word\":\n",
    "        return key_tokenized_words\n",
    "    elif op == \"clean_sent\":\n",
    "        return [clean(i) for i in key_tokenized_sentences]\n",
    "    elif op == \"clean_word\":\n",
    "        return [clean(i) for i in key_tokenized_words]\n",
    "    elif op == \"lem_sent\":\n",
    "        key_clean_sentences = pp_set(text, \"clean_sent\")\n",
    "        return [\" \".join([lemmatizer.lemmatize(j) for j in i.split()]) for i in key_clean_sentences]\n",
    "    elif op == \"lem_word\":\n",
    "        key_clean_words = pp_set(text, \"clean_word\")\n",
    "        return [lemmatizer.lemmatize(i) for i in key_clean_words]\n",
    "    elif op == \"prep_sent\":\n",
    "        key_clean_sentences = pp_set(text, \"clean_sent\")\n",
    "        return [\" \".join([i for i in j.split() if i not in stopw]) for j in key_clean_sentences]\n",
    "    elif op == \"prep_word\":\n",
    "        key_preprocessed_sentences = pp_set(text, \"prep_sent\")\n",
    "        key_preprocessed_words = []\n",
    "        for i in key_preprocessed_sentences:\n",
    "            key_preprocessed_words.extend(word_tokenize(i))\n",
    "        return key_preprocessed_words\n",
    "    elif op == \"pp_lem_word\":\n",
    "        return [lemmatizer.lemmatize(i) for i in pp_set(text, \"prep_word\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_keywords(text):\n",
    "    r.extract_keywords_from_sentences(pp_set(text, \"lem_sent\")) # Effectiveness : tokenized > lemmatized > clean \n",
    "    rake_keywords = r.get_ranked_phrases()\n",
    "    spdoc = sp(text)\n",
    "    ner_keywords = []\n",
    "    for ent in spdoc.ents:\n",
    "        ner_keywords.append(ent.text)\n",
    "    spdoc = sp(\" \".join(pp_set(text, \"clean_word\")))\n",
    "    pytr_keywords = []\n",
    "    for p in spdoc._.phrases:\n",
    "        for term in p.chunks:\n",
    "            if term.text not in pytr_keywords and term.text not in stopw:\n",
    "                x = term.text\n",
    "                pytr_keywords.append(x)\n",
    "    nounchunk_keywords = list(set([i.text for i in set(spdoc.noun_chunks) if i.text not in stopw and len(min(i.text.split(),key = len))>1]))\n",
    "    \n",
    "    bigrams = list(ngrams(pp_set(text, \"clean_word\"),2))\n",
    "    trigrams = list(ngrams(pp_set(text, \"clean_word\"),3))\n",
    "    quadgrams = list(ngrams(pp_set(text, \"clean_word\"),4))\n",
    "    pentagrams = list(ngrams(pp_set(text, \"clean_word\"),5))\n",
    "    quadgrams = list(set([\" \".join([j for j in i if j not in stopw]).strip() for i in quadgrams]))\n",
    "    #quadgrams = [i for i in quadgrams if i not in stopw and i!='' and len(i.split())>1]\n",
    "    ngram_vector_key = dict()\n",
    "    key_clean_sentences = pp_set(text, \"clean_sent\")\n",
    "    for i in quadgrams:\n",
    "        ngram_vector_key[i] = [0 for i in range(len(key_clean_sentences))]\n",
    "    for i in range(len(key_clean_sentences)):\n",
    "        for phrase in ngram_vector_key:\n",
    "            ngram_vector_key[phrase][i] = (key_clean_sentences[i].count(phrase)/len(word_tokenize(key_clean_sentences[i])))\n",
    "            df = 0\n",
    "            for j in key_clean_sentences:\n",
    "                if phrase in j:\n",
    "                    df+=1\n",
    "            ngram_vector_key[phrase][i]*=(1+log((len(key_clean_sentences)+1)/(df+1)))\n",
    "    ngram_keywords = sorted(ngram_vector_key,key = lambda x:sum(ngram_vector_key[x]),reverse = True)\n",
    "    \n",
    "    \n",
    "    \n",
    "    all_keywords = rake_keywords+pytr_keywords+ner_keywords+nounchunk_keywords#+ngram_keywords\n",
    "    all_keywords = list(set(all_keywords))\n",
    "    sorted_keywords = list(all_keywords)\n",
    "    sorted_keywords.sort()\n",
    "    for i in range(len(sorted_keywords)):\n",
    "        sorted_keywords[i] = re.sub(r' +',' ',sorted_keywords[i])\n",
    "    \n",
    "    return sorted_keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def group(sorted_keywords):\n",
    "    grouped_keys = []\n",
    "    for i in sorted_keywords:\n",
    "        if len(grouped_keys)==0:\n",
    "            grouped_keys.append([i])\n",
    "            continue\n",
    "        else:\n",
    "            flag = False\n",
    "            for j in grouped_keys:\n",
    "                if i in j:\n",
    "                    flag = True\n",
    "                    break\n",
    "                temp1 = \" \".join([lemmatizer.lemmatize(t) for t in stopwordremoval(i).split()])\n",
    "                for k in j:\n",
    "                    temp2 = \" \".join([lemmatizer.lemmatize(t) for t in stopwordremoval(k).split()])\n",
    "                    short = min(temp1,temp2)\n",
    "                    long = max(temp1,temp2)\n",
    "                    if short in long:\n",
    "                        flag = True\n",
    "                        j.append(i)\n",
    "                        break\n",
    "                if flag == True:\n",
    "                    break            \n",
    "            if flag==False:\n",
    "                grouped_keys.append([i])\n",
    "    temp = []\n",
    "    for i in grouped_keys:\n",
    "        k = sorted(i,key = len)\n",
    "        temp.append(k)\n",
    "    return temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_duplicates(grouped_keys):    \n",
    "    for i in range(len(grouped_keys)):\n",
    "        grouped_keys[i] = list(set(grouped_keys[i]))\n",
    "        temp = list(grouped_keys[i])\n",
    "        process_set = [\" \".join([lemmatizer.lemmatize(l) for l in stopwordremoval(j).split()]) for j in grouped_keys[i]]\n",
    "        process_set = list(set(process_set))\n",
    "        for temp_key1 in grouped_keys[i]:\n",
    "            x = \" \".join([lemmatizer.lemmatize(k) for k in stopwordremoval(temp_key1).split()])\n",
    "            if process_set.count(x)>1:\n",
    "                temp.remove(temp_key1)   \n",
    "        grouped_keys[i] = temp\n",
    "        grouped_keys[i] = sorted(grouped_keys[i])\n",
    "        \n",
    "        \n",
    "    for i in range(len(grouped_keys)):\n",
    "        temp = list(grouped_keys[i])\n",
    "        for j in range(len(grouped_keys[i])):\n",
    "            word = grouped_keys[i][j]\n",
    "            for k in temp:\n",
    "                if word in k and word!=k:\n",
    "                    temp.remove(word)\n",
    "                    break\n",
    "        grouped_keys[i] = sorted(temp,key = len, reverse = True)\n",
    "    grouped_keys = [i for i in grouped_keys if len(i)>0]\n",
    "    return grouped_keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def finalize(grouped_keys):\n",
    "    temp_keywords = []\n",
    "    final_keywords = []\n",
    "    for i in grouped_keys:\n",
    "        for j in i:\n",
    "            temp_keywords.append(j)\n",
    "    \n",
    "    temp_keywords = remove_duplicates(group(temp_keywords))\n",
    "    \n",
    "    for i in temp_keywords:\n",
    "        for j in i:\n",
    "            final_keywords.append(j)\n",
    "    return final_keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a salt',\n",
       " 'bihar cross bun',\n",
       " 'bring',\n",
       " 'call dip restore selfinvest',\n",
       " 'my favourite',\n",
       " 'find',\n",
       " 'food',\n",
       " 'the order',\n",
       " 'the salesman old beer drinkers',\n",
       " 'selfinvest',\n",
       " 'take hi']"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keywords = finalize(remove_duplicates(group(extract_keywords(query.lower()))))\n",
    "keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
