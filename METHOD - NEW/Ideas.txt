video   -> audio -> transcript -> audio_keywords
	-> frames -> objects, description
 
description:
https://machinelearningmastery.com/develop-a-deep-learning-caption-generation-model-in-python/
https://www.analyticsvidhya.com/blog/2018/04/solving-an-image-captioning-task-using-deep-learning/
https://towardsdatascience.com/image-captioning-with-keras-teaching-computers-to-describe-pictures-c88a46a311b8

filtered frames - 
	1. YouTube: 
		if YT link present -> extract keywords from title, description
	2. use description of video 

	// CHUCKED FOR NOW
	3. description of video (for each frame) 
		-> keywords -> Google image search -> relevant searches
	4. Google image titles - VERY BAD SEARCHES

filtering needed (list(set(all_keywords)) -> clustering/topK) -> filtered_keywords

1. audio keywords
2. frame objects
3. keywords from frame description

frame_keywords = [[f1k1,f1k2,...,f1kn], [f2k1...f2kn], [fnk1...fnkn]]
topK -> topK(frame_keywords)
topK words:
	1. Get keywords returned for every frame
	2. Return top K repeating words

topK -> clustering(topKwords+audio_keywords or topKwords)
clustering:
	1. words in vocabulary - we are good!
	2. words not in vocabulary - what do we do?
		2.1. Assign 0 - all absent words get assigned 0 [large set of irrelevant stuff]
		2.2. Assign random - BAD CHOICE, but better than 0
		2.3. Remove these words - remove keywords, but obtain accurate cluster
		2.4. Keep these words - keep relevant as well as irrelevant


final tags: 
	1. NER (must)
	2. filtered_keywords